import os
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ====================== 1. åœç”¨è¯é…ç½® ======================
def load_stopwords(stopwords_path: str):
    """åŠ è½½æœ¬åœ°åœç”¨è¯æ–‡ä»¶"""
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f if line.strip())
        print(f"âœ… æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")
        return stopwords
    except Exception as e:
        print(f"âš ï¸ åŠ è½½åœç”¨è¯å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨é»˜è®¤åœç”¨è¯")
        return {"çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ"}  # åŸºç¡€ä¸­æ–‡åœç”¨è¯


# é…ç½®åœç”¨è¯ï¼ˆæ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
STOPWORDS_PATH = "/Users/yxy/work/python/ai-demo4-my/stop-words/cn_stopwords.txt"  # å»ºè®®ä½¿ç”¨æ¸…åå¤§å­¦åœç”¨è¯åº“
Settings.stopwords = load_stopwords(STOPWORDS_PATH)
Settings.tokenizer = None  # ç¦ç”¨NLTKåˆ†è¯å™¨

# ====================== 2. æ¨¡å‹é…ç½® ======================
# æ»‘åŠ¨çª—å£è§£æå™¨
node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key="window",
    original_text_metadata_key="original_text",
)

# BGEåµŒå…¥æ¨¡å‹
embed_model = HuggingFaceEmbedding(
    model_name="/Users/yxy/work/ai/model-new/bge-small-zh-v1.5",
    embed_batch_size=32
)


# ====================== 3. æ–‡æ¡£å¤„ç†æ ¸å¿ƒé€»è¾‘ ======================
def ingest_documents(directory_path: str, collection_name: str):
    try:
        # è¯»å–æ–‡æ¡£
        reader = SimpleDirectoryReader(
            input_dir=directory_path,
            recursive=True,
            required_exts=[".pdf", ".docx", ".pptx", ".txt", ".md"]
        )
        documents = reader.load_data()
        print(f"ğŸ“„ æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # æ»‘åŠ¨çª—å£åˆ†å‰²
        nodes = node_parser.get_nodes_from_documents(documents)
        print(f"âœ‚ï¸ åˆ†å‰²ä¸º {len(nodes)} ä¸ªæ–‡æœ¬å—ï¼ˆå·²è‡ªåŠ¨åº”ç”¨åœç”¨è¯è¿‡æ»¤ï¼‰")

        # è¿æ¥ChromaDB
        chroma_client = chromadb.HttpClient(host="localhost", port=8000)
        chroma_client.delete_collection(collection_name);
        vector_store = ChromaVectorStore(
            chroma_collection=chroma_client.get_or_create_collection(collection_name)
        )

        # æ„å»ºç´¢å¼•
        index = VectorStoreIndex(
            nodes=nodes,
            storage_context=StorageContext.from_defaults(vector_store=vector_store),
            embed_model=embed_model,
            show_progress=True
        )

        print(f"\nâœ… å®Œæˆï¼å·²å­˜å‚¨ {len(nodes)} ä¸ªèŠ‚ç‚¹åˆ° '{collection_name}'")
        print(f"ğŸ”— ChromaDBå¯è§†åŒ–: http://localhost:8000")

    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        raise


# ====================== 4. ä¸»ç¨‹åº ======================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    DOC_DIR = "/Users/yxy/work/python/ai-demo4-my/data"  # ä½ çš„æ–‡æ¡£ç›®å½•
    COLLECTION_NAME = "knowledge_base_v2"  # ChromaDBé›†åˆå

    # è¿è¡Œå¤„ç†
    ingest_documents(DOC_DIR, COLLECTION_NAME)
