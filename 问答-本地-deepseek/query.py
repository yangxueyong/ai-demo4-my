import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)
import chromadb
from rich.console import Console
from rich.prompt import Prompt
from rich.markdown import Markdown

# åˆå§‹åŒ–æ§åˆ¶å°
console = Console()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ====================== 1. é…ç½®æœ¬åœ°DeepSeekæ¨¡å‹ ======================
def load_local_deepseek(model_path: str):
    """åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹ (GGUFé‡åŒ–æ ¼å¼)"""
    try:
        console.print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹...", style="yellow")

        # é…ç½®æœ¬åœ°æ¨¡å‹å‚æ•°
        llm = LlamaCPP(
            model_path=model_path,
            temperature=0.3,
            max_new_tokens=1024,
            context_window=2048,
            model_kwargs={
                "n_gpu_layers": 20,  # å¢åŠ MetalåŠ é€Ÿå±‚æ•°(M1/M2å»ºè®®20-40)
                "main_gpu": 0,  # æ˜ç¡®ä½¿ç”¨GPU
            },
            messages_to_prompt=messages_to_prompt,
            completion_to_prompt=completion_to_prompt,
            verbose=False,
        )

        Settings.llm = llm
        console.print("âœ… æœ¬åœ°DeepSeekæ¨¡å‹åŠ è½½æˆåŠŸ", style="green")
        return llm
    except Exception as e:
        console.print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 2. åˆå§‹åŒ–é—®ç­”å¼•æ“ ======================
def initialize_query_engine(collection_name: str):
    """åˆå§‹åŒ–ChromaDBæŸ¥è¯¢å¼•æ“"""
    try:
        console.print("ğŸ”„ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...", style="yellow")

        # è¿æ¥ChromaDB
        chroma_client = chromadb.HttpClient(host="localhost", port=8000)
        chroma_collection = chroma_client.get_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model="local:/Users/yxy/work/ai/model-new/bge-small-zh-v1.5"
        )

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = index.as_query_engine(
            similarity_top_k=2,
            response_mode="compact",
            verbose=True
        )
        # query_engine = index.as_query_engine(
        #     similarity_top_k=2,  # å‡å°‘è¿”å›ç»“æœæ•°é‡
        #     response_mode="refine",  # å°è¯•ä¸åŒçš„responseæ¨¡å¼
        #     verbose=True,
        #     streaming=True  # å¯ç”¨æµå¼è¾“å‡º
        # )

        console.print("âœ… å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ", style="green")
        return query_engine
    except Exception as e:
        console.print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 3. äº¤äº’å¼é—®ç­”å¾ªç¯ ======================
def chat_loop(query_engine):
    """äº¤äº’å¼é—®ç­”å¾ªç¯"""
    console.print("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'æˆ–'quit'ç»“æŸï¼‰", style="bold blue")

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = Prompt.ask("â“ é—®é¢˜")

            # é€€å‡ºæ¡ä»¶
            if query.lower() in ["é€€å‡º", "quit", "exit"]:
                console.print("ğŸ‘‹ å†è§ï¼", style="bold green")
                break

            if not query.strip():
                console.print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜", style="yellow")
                continue

            # æ‰§è¡ŒæŸ¥è¯¢
            console.print("ğŸ”„ æ­£åœ¨æ€è€ƒ...", style="yellow")
            response = query_engine.query(query)

            # æ˜¾ç¤ºç»“æœ
            console.print("\nğŸ“š å›ç­”:", style="bold green")
            console.print(Markdown(str(response)))

            # æ˜¾ç¤ºæ¥æº
            if hasattr(response, 'source_nodes'):
                console.print("\nğŸ” æ¥æºå‚è€ƒ:", style="bold blue")
                for i, node in enumerate(response.source_nodes, 1):
                    console.print(f"{i}. {node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}")
                    console.print(f"   [ç›¸ä¼¼åº¦: {node.score:.3f}]")
                    console.print("-" * 50)

        except KeyboardInterrupt:
            console.print("\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œå†è§ï¼", style="bold red")
            break
        except Exception as e:
            console.print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", style="red")


# ====================== 4. ä¸»ç¨‹åº ======================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    DEEPSEEK_MODEL_PATH = "/Users/yxy/work/ai/model-new/deepseek/deepseek-llm-7b-chat.Q4_K_M.gguf"  # å¿…é¡»ä½¿ç”¨GGUFæ ¼å¼
    COLLECTION_NAME = "knowledge_base_v2"

    try:
        # åˆå§‹åŒ–æ¨¡å‹å’Œå¼•æ“
        load_local_deepseek(DEEPSEEK_MODEL_PATH)
        query_engine = initialize_query_engine(COLLECTION_NAME)

        # å¯åŠ¨é—®ç­”
        chat_loop(query_engine)

    except Exception as e:
        console.print(f"\nâŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}", style="red")
