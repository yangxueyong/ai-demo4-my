import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)
import chromadb
from rich.console import Console
from rich.prompt import Prompt
from rich.markdown import Markdown

# åˆå§‹åŒ–æ§åˆ¶å°
console = Console()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ====================== 1. é…ç½®æœ¬åœ°DeepSeekæ¨¡å‹ ======================
def load_local_deepseek(model_path: str):
    """åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹ (GGUFé‡åŒ–æ ¼å¼)"""
    try:
        console.print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹...", style="yellow")

        # é…ç½®æœ¬åœ°æ¨¡å‹å‚æ•°
        llm = LlamaCPP(
            model_path=model_path,
            temperature=0.3,
            max_new_tokens=512,  # å‡å°‘æœ€å¤§tokenæ•°
            context_window=2048,
            model_kwargs={
                "n_gpu_layers": 40,  # å¢åŠ MetalåŠ é€Ÿå±‚æ•°
                "main_gpu": 0,
                "n_threads": 6,  # æ˜ç¡®è®¾ç½®çº¿ç¨‹æ•°
                "n_batch": 512,  # å¢åŠ æ‰¹å¤„ç†å¤§å°
                "use_mmap": True,  # å¯ç”¨å†…å­˜æ˜ å°„
                "use_mlock": True  # é”å®šå†…å­˜é˜²æ­¢äº¤æ¢
            },
            messages_to_prompt=messages_to_prompt,
            completion_to_prompt=completion_to_prompt,
            verbose=False,
        )

        Settings.llm = llm
        console.print("âœ… æœ¬åœ°DeepSeekæ¨¡å‹åŠ è½½æˆåŠŸ", style="green")
        return llm
    except Exception as e:
        console.print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 2. åˆå§‹åŒ–é—®ç­”å¼•æ“ ======================
def initialize_query_engine(collection_name: str):
    """åˆå§‹åŒ–ChromaDBæŸ¥è¯¢å¼•æ“"""
    try:
        console.print("ğŸ”„ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...", style="yellow")

        # è¿æ¥ChromaDB
        chroma_client = chromadb.HttpClient(host="localhost", port=8000)
        chroma_collection = chroma_client.get_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model="local:/Users/yxy/work/ai/model-new/bge-small-zh-v1.5"
        )

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        # query_engine = index.as_query_engine(
        #     similarity_top_k=2,
        #     response_mode="compact",
        #     verbose=True
        # )
        query_engine = index.as_query_engine(
            similarity_top_k=2,
            vector_store_query_mode="hybrid",  # æ··åˆæ£€ç´¢æ¨¡å¼
            alpha=0.5,  # å¹³è¡¡å…³é”®å­—å’Œå‘é‡æœç´¢
            response_mode="compact",
            streaming=True,
            verbose=False  # å…³é—­è¯¦ç»†æ—¥å¿—
        )

        console.print("âœ… å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ", style="green")
        return query_engine
    except Exception as e:
        console.print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 3. äº¤äº’å¼é—®ç­”å¾ªç¯ ======================
# ====================== 3. äº¤äº’å¼é—®ç­”å¾ªç¯ ======================
def chat_loop(query_engine):
    """äº¤äº’å¼é—®ç­”å¾ªç¯"""
    console.print("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'æˆ–'quit'ç»“æŸï¼‰", style="bold blue")

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = Prompt.ask("â“ é—®é¢˜")

            # é€€å‡ºæ¡ä»¶
            if query.lower() in ["é€€å‡º", "quit", "exit"]:
                console.print("ğŸ‘‹ å†è§ï¼", style="bold green")
                break

            if not query.strip():
                console.print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜", style="yellow")
                continue

            # æ‰§è¡ŒæŸ¥è¯¢
            console.print("ğŸ”„ æ­£åœ¨æ€è€ƒ...", style="yellow")

            # æ˜¾ç¤ºå›ç­”æ ‡é¢˜
            console.print("\nğŸ“š å›ç­”:", style="bold green")

            try:
                # è·å–æµå¼å“åº”
                streaming_response = query_engine.query(query)

                # æ£€æŸ¥æ˜¯å¦æœ‰æµå¼ç”Ÿæˆå™¨
                if not hasattr(streaming_response, 'response_gen'):
                    console.print(str(streaming_response), style="green")
                    continue

                # åˆ›å»ºä¸€ä¸ªç©ºçš„å“åº”å­—ç¬¦ä¸²æ¥ç´¯ç§¯å†…å®¹
                full_response = ""

                # å¤„ç†å¯èƒ½çš„ç”Ÿæˆå™¨å·²æ‰§è¡Œé”™è¯¯
                try:
                    # é€å­—è¾“å‡ºå“åº”
                    for token in streaming_response.response_gen:
                        if token:  # ç¡®ä¿tokenä¸ä¸ºç©º
                            console.print(token, end="", style="green")
                            full_response += token
                            # ç«‹å³åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
                            console.file.flush()

                    # æ·»åŠ æ¢è¡Œç¬¦
                    console.print()

                    # ä¿å­˜å®Œæ•´å“åº”ä»¥ä¾¿åç»­å¤„ç†
                    streaming_response.response = full_response

                except RuntimeError as e:
                    if "generator already executing" in str(e):
                        console.print("\nâš ï¸ æµå¼å“åº”ä¸­æ–­ï¼Œè½¬ä¸ºå®Œæ•´æ˜¾ç¤º", style="yellow")
                        console.print(full_response, style="green")
                    else:
                        raise

                # æ˜¾ç¤ºæ¥æºï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(streaming_response, 'source_nodes'):
                    console.print("\nğŸ” æ¥æºå‚è€ƒ:", style="bold blue")
                    for i, node in enumerate(streaming_response.source_nodes, 1):
                        console.print(f"{i}. {node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}")
                        console.print(f"   [ç›¸ä¼¼åº¦: {node.score:.3f}]")
                        console.print("-" * 50)

            except Exception as e:
                console.print(f"\nâŒ å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}", style="red")
                continue

        except KeyboardInterrupt:
            console.print("\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œå†è§ï¼", style="bold red")
            break
        except Exception as e:
            console.print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", style="red")


# ====================== 4. ä¸»ç¨‹åº ======================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    DEEPSEEK_MODEL_PATH = "/Users/yxy/work/ai/model-new/deepseek/deepseek-coder-1.3b-instruct.Q4_K_M.gguf"  # å¿…é¡»ä½¿ç”¨GGUFæ ¼å¼
    COLLECTION_NAME = "knowledge_base_v2"

    try:
        # åˆå§‹åŒ–æ¨¡å‹å’Œå¼•æ“
        load_local_deepseek(DEEPSEEK_MODEL_PATH)
        query_engine = initialize_query_engine(COLLECTION_NAME)

        # å¯åŠ¨é—®ç­”
        chat_loop(query_engine)

    except Exception as e:
        console.print(f"\nâŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}", style="red")
