import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.llms.llama_cpp.llama_utils import completion_to_prompt
import chromadb
from rich.console import Console
from rich.prompt import Prompt
from llama_index.core.postprocessor import (  # ä¿®æ”¹å¯¼å…¥æ–¹å¼
    SimilarityPostprocessor,
    KeywordNodePostprocessor
)

# åˆå§‹åŒ–æ§åˆ¶å°
console = Console()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ====================== 1. è‡ªå®šä¹‰ä¸­æ–‡æç¤ºæ¨¡æ¿ ======================
def messages_to_prompt_chinese(messages):
    """å¼ºåˆ¶ä½¿ç”¨ä¸­æ–‡çš„è‡ªå®šä¹‰æç¤ºæ¨¡æ¿"""
    system_prompt = {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡AIåŠ©æ‰‹ï¼Œå¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚å›ç­”æ—¶éœ€æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š"
                   "\n1. ä¸¥æ ¼ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œç¦æ­¢è¾“å‡ºä»»ä½•è‹±æ–‡å†…å®¹"
                   "\n2. åªåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸æ·»åŠ æ— å…³ä¿¡æ¯"
                   "\n3. å¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªä¸»é¢˜ï¼Œè¯·æ˜ç¡®åŒºåˆ†"
                   "\n4. å›ç­”å†…å®¹è¦ç®€æ´å‡†ç¡®ï¼Œé¿å…é‡å¤"
    }

    # å°†ç³»ç»Ÿæç¤ºæ’å…¥åˆ°æ¶ˆæ¯å¼€å¤´
    if messages[0]["role"] != "system":
        messages = [system_prompt] + messages

    prompt = ""
    for message in messages:
        if message["role"] == "system":
            prompt += f"<|system|>\n{message['content']}</s>\n"
        elif message["role"] == "user":
            prompt += f"<|user|>\n{message['content']}</s>\n"
        elif message["role"] == "assistant":
            prompt += f"<|assistant|>\n{message['content']}</s>\n"

    # ç¡®ä¿æœ€åä»¥åŠ©ç†å“åº”å¼€å¤´ç»“æŸ
    if not prompt.endswith("<|assistant|>\n"):
        prompt += "<|assistant|>\n"

    return prompt


# ====================== 2. é…ç½®æœ¬åœ°DeepSeekæ¨¡å‹ ======================
def load_local_deepseek(model_path: str):
    """åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹ (GGUFé‡åŒ–æ ¼å¼)"""
    try:
        console.print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°DeepSeekæ¨¡å‹...", style="yellow")

        # é…ç½®æœ¬åœ°æ¨¡å‹å‚æ•°
        llm = LlamaCPP(
            model_path=model_path,
            temperature=0.3,
            max_new_tokens=1024,
            context_window=16384,  # å¢å¤§ä¸Šä¸‹æ–‡çª—å£
            model_kwargs={
                "n_gpu_layers": 40,  # å¢åŠ MetalåŠ é€Ÿå±‚æ•°
                "main_gpu": 0,
                "f16_kv": False,
                "n_threads": 6,  # è®¾ç½®çº¿ç¨‹æ•°
                "n_batch": 512,  # æ‰¹å¤„ç†å¤§å°
                "use_mmap": True,  # å†…å­˜æ˜ å°„
                "use_mlock": True  # é”å®šå†…å­˜
            },
            messages_to_prompt=messages_to_prompt_chinese,  # ä½¿ç”¨ä¸­æ–‡æç¤ºæ¨¡æ¿
            completion_to_prompt=completion_to_prompt,
            verbose=False,
        )
        Settings.tokenizer = None
        Settings.llm = llm
        console.print("âœ… æœ¬åœ°DeepSeekæ¨¡å‹åŠ è½½æˆåŠŸ", style="green")
        return llm
    except Exception as e:
        console.print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 3. åˆå§‹åŒ–é—®ç­”å¼•æ“ ======================
def initialize_query_engine(collection_name: str):
    """åˆå§‹åŒ–ChromaDBæŸ¥è¯¢å¼•æ“"""
    try:
        console.print("ğŸ”„ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...", style="yellow")

        # è¿æ¥ChromaDB
        chroma_client = chromadb.HttpClient(host="localhost", port=8000)
        chroma_collection = chroma_client.get_collection(collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model="local:/Users/yxy/work/ai/model-new/bge-small-zh-v1.5"
        )

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        # ä¿®æ”¹initialize_query_engineå‡½æ•°ä¸­çš„æŸ¥è¯¢å¼•æ“é…ç½®
        query_engine = index.as_query_engine(
            similarity_top_k=3,
            vector_store_query_mode="hybrid",
            alpha=0.6,  # å¢åŠ å‘é‡æœç´¢æƒé‡
            response_mode="compact",
            streaming=True,
            verbose=False,
            # node_postprocessors=[
            #     # ç»„åˆå¤šä¸ªåå¤„ç†å™¨
            #     KeywordNodePostprocessor(),  # ç¡®ä¿åŸºç¡€å¤„ç†
            #     SimilarityPostprocessor(
            #         similarity_cutoff=0.3  # é™ä½é˜ˆå€¼
            #     )
            # ]
        )

        console.print("âœ… å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ", style="green")
        return query_engine
    except Exception as e:
        console.print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}", style="red")
        raise


# ====================== 4. äº¤äº’å¼é—®ç­”å¾ªç¯ ======================
def chat_loop(query_engine):
    """äº¤äº’å¼é—®ç­”å¾ªç¯"""
    console.print("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'é€€å‡º'æˆ–'quit'ç»“æŸï¼‰", style="bold blue")

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            query = Prompt.ask("â“ é—®é¢˜")

            # é€€å‡ºæ¡ä»¶
            if query.lower() in ["é€€å‡º", "quit", "exit"]:
                console.print("ğŸ‘‹ å†è§ï¼", style="bold green")
                break

            if not query.strip():
                console.print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜", style="yellow")
                continue

            # æ‰§è¡ŒæŸ¥è¯¢
            console.print("ğŸ”„ æ­£åœ¨æ€è€ƒ...", style="yellow")
            console.print("\nğŸ“š å›ç­”:", style="bold green")

            try:
                # è·å–æµå¼å“åº”
                streaming_response = query_engine.query(query)

                # å¤„ç†å“åº”
                full_response = ""
                for token in streaming_response.response_gen:
                    console.print(token, end="", style="green")
                    full_response += token
                    console.file.flush()

                console.print()  # æ¢è¡Œ

                # æ˜¾ç¤ºæ¥æº
                if hasattr(streaming_response, 'source_nodes'):
                    console.print("\nğŸ” æ¥æºå‚è€ƒ:", style="bold blue")
                    for i, node in enumerate(streaming_response.source_nodes, 1):
                        source_info = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                        console.print(f"{i}. {source_info} [ç›¸ä¼¼åº¦: {node.score:.3f}]")

            except Exception as e:
                console.print(f"\nâŒ å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}", style="red")

        except KeyboardInterrupt:
            console.print("\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œå†è§ï¼", style="bold red")
            break
        except Exception as e:
            console.print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", style="red")


# ====================== 5. ä¸»ç¨‹åº ======================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    DEEPSEEK_MODEL_PATH = "/Users/yxy/work/ai/model-new/deepseek/deepseek-coder-1.3b-instruct.Q8_0.gguf"
    COLLECTION_NAME = "knowledge_base_v2"

    try:
        # åˆå§‹åŒ–æ¨¡å‹å’Œå¼•æ“
        console.print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...", style="bold blue")
        load_local_deepseek(DEEPSEEK_MODEL_PATH)
        query_engine = initialize_query_engine(COLLECTION_NAME)

        # é¢„çƒ­æ¨¡å‹
        console.print("ğŸ”¥ é¢„çƒ­æ¨¡å‹ä¸­...", style="yellow")
        query_engine.query("ä½ å¥½")

        # å¯åŠ¨é—®ç­”
        console.print("ğŸš€ ç³»ç»Ÿå‡†å¤‡å°±ç»ª", style="bold green")
        chat_loop(query_engine)

    except Exception as e:
        console.print(f"\nâŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}", style="red")
